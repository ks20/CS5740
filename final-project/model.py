# -*- coding: utf-8 -*-
"""NLP_Final.ipynb

Automatically generated by Colaboratory.

"""

from collections import Counter, OrderedDict

import torch
import torch.nn as nn
import numpy as np
import tqdm
import nltk

nltk.download('averaged_perceptron_tagger')

UNK = '<unk>'
PAD = '<pad>'
NULL = 'NULL'
START_TAG = "<START>"
STOP_TAG = "<STOP>"

from google.colab import drive
drive.mount('/content/drive')

train_label_path = "/content/drive/My Drive/NLP Assignments/Final/data/train/train.txt"
dev_label_path = "/content/drive/My Drive/NLP Assignments/Final/data/dev/dev.txt"
test_path = "/content/drive/My Drive/NLP Assignments/Final/data/test/test.nolabels.txt"

word_to_id, char_to_id = {}, {}
label_to_id = {START_TAG:0, 'O': 1, 'B': 2, 'I': 3, STOP_TAG:4}
id_to_label = {0: START_TAG, 1: 'O', 2: 'B', 3: 'I', 4: STOP_TAG}

def build_vocab(sentences, tags, labels):
    global word_to_id
    word_counts = Counter()
    idx = 0
    for sentence in sentences:
        for word in sentence:
            word_counts[word] += 1

            for char in word:
                if char not in char_to_id:
                    char_to_id[char] = idx
                    idx += 1
    char_to_id[UNK] = idx
    sorted_word_counts = word_counts.most_common()

    word_to_id = {sorted_word_counts[i][0]: i for i in range(0, len(sorted_word_counts))}
    word_to_id[UNK] = len(word_to_id)

def build_sentences():
    train_sentences, train_labels, train_tags = list(), list(), list()
    dev_sentences, dev_labels, dev_tags = list(), list(), list()
    test_sentences, test_labels, test_tags = list(), list(), list()
    train_sentence, dev_sentence, test_sentence = list(), list(), list()
    train_label, dev_label, test_label = list(), list(), list()

    train = open(train_label_path, encoding='utf-8')
    dev = open(dev_label_path, encoding='utf-8')
    test = open(test_path, encoding='utf-8')

    for train_line in train:
      train_line = train_line.split()
      if train_line:
          train_word, train_word_label = train_line[0], train_line[1]
          train_sentence.append(train_word.lower())
          train_label.append(train_word_label)
      else:
          train_sentences.append(train_sentence)
          train_labels.append(train_label)
          train_tags.append([tag for word, tag in nltk.pos_tag(train_sentence)])
          train_sentence, train_label = list(), list()

    for dev_line in dev:
      dev_line = dev_line.split()
      if dev_line:
          dev_word, dev_word_label = dev_line[0], dev_line[1]
          dev_sentence.append(dev_word.lower())
          dev_label.append(dev_word_label)
      else:
          dev_sentences.append(dev_sentence)
          dev_labels.append(dev_label)
          dev_tags.append([tag for word, tag in nltk.pos_tag(dev_sentence)])
          dev_sentence, dev_label = list(), list()

    for test_line in test:
      test_line = test_line.strip()
      if test_line:
          test_sentence.append(test_line.lower())
      else:
          test_sentences.append(test_sentence)
          test_tags.append([tag for word, tag in nltk.pos_tag(test_sentence)])
          test_sentence = list()

    return train_sentences, train_labels, train_tags, dev_sentences, dev_labels, dev_tags, test_sentences, test_labels, test_tags

# Code implementation courtesy of https://github.com/ZhixiuYe/NER-pytorch
def prepare_dataset(sentences, labels, word_to_id, char_to_id, tag_to_id, lower=True):
    def f(x): return x.lower() if lower else x
    data = []
    for i in range(len(sentences)):
        s = sentences[i]
        str_words = [w for w in s]
        words = [word_to_id[f(w) if f(w) in word_to_id else UNK]
                 for w in str_words]
        chars = [[char_to_id[c if c in char_to_id else UNK] for c in w] for w in str_words]
        caps = [cap_feature(w) for w in str_words]
        if len(labels) != 0:
          tags = [tag_to_id[w] for w in labels[i]]
        else: 
          tags=[]
        data.append({
            'str_words': str_words,
            'words': words,
            'chars': chars,
            'caps': caps,
            'tags': tags,
        })
    return data

def cap_feature(s):
    if s.lower() == s:
        return 0 # low caps
    elif s.upper() == s:
        return 1 # all caps
    elif s[0].upper() == s[0]:
        return 2 # first letter cap
    else:
        return 3 # one capital (not first)

# Code implementation courtesy of: https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html, https://github.com/ZhixiuYe/NER-pytorch/
def argmax(vec):
    _, idx = torch.max(vec, 1)
    return idx.item()

def log_sum_exp(vec):
    max_score = vec[0, argmax(vec)]
    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])
    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))

def init_embedding(input_embedding):
    bias = np.sqrt(3.0 / input_embedding.size(1))
    nn.init.uniform(input_embedding, -bias, bias)

def init_linear(input_linear):
    bias = np.sqrt(6.0 / (input_linear.weight.size(0) + input_linear.weight.size(1)))
    nn.init.uniform(input_linear.weight, -bias, bias)
    if input_linear.bias is not None:
        input_linear.bias.data.zero_()

def init_lstm(input_lstm):
    for ind in range(0, input_lstm.num_layers):
        weight = eval('input_lstm.weight_ih_l' + str(ind))
        bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))
        nn.init.uniform(weight, -bias, bias)
        weight = eval('input_lstm.weight_hh_l' + str(ind))
        bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))
        nn.init.uniform(weight, -bias, bias)
    if input_lstm.bidirectional:
        for ind in range(0, input_lstm.num_layers):
            weight = eval('input_lstm.weight_ih_l' + str(ind) + '_reverse')
            bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))
            nn.init.uniform(weight, -bias, bias)
            weight = eval('input_lstm.weight_hh_l' + str(ind) + '_reverse')
            bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))
            nn.init.uniform(weight, -bias, bias)

    if input_lstm.bias:
        for ind in range(0, input_lstm.num_layers):
            weight = eval('input_lstm.bias_ih_l' + str(ind))
            weight.data.zero_()
            weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1
            weight = eval('input_lstm.bias_hh_l' + str(ind))
            weight.data.zero_()
            weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1
        if input_lstm.bidirectional:
            for ind in range(0, input_lstm.num_layers):
                weight = eval('input_lstm.bias_ih_l' + str(ind) + '_reverse')
                weight.data.zero_()
                weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1
                weight = eval('input_lstm.bias_hh_l' + str(ind) + '_reverse')
                weight.data.zero_()
                weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1

class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, char_lstm_dim=25,
                 char_to_ix=None, pre_word_embeds=None, char_embedding_dim=25, use_gpu=False,
                 n_cap=None, cap_embedding_dim=None, use_crf=True, char_mode='LSTM'):
        super(BiLSTM_CRF, self).__init__()
        self.use_gpu = use_gpu
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.vocab_size = vocab_size
        self.tag_to_ix = tag_to_ix
        self.n_cap = n_cap
        self.cap_embedding_dim = cap_embedding_dim
        self.use_crf = use_crf
        self.tagset_size = len(tag_to_ix)
        self.out_channels = char_lstm_dim
        self.char_mode = char_mode

        if self.n_cap and self.cap_embedding_dim:
            self.cap_embeds = nn.Embedding(self.n_cap, self.cap_embedding_dim)
            init_embedding(self.cap_embeds.weight)

        if char_embedding_dim is not None:
            self.char_lstm_dim = char_lstm_dim
            self.char_embeds = nn.Embedding(len(char_to_ix), char_embedding_dim)
            init_embedding(self.char_embeds.weight)
            self.char_lstm = nn.LSTM(char_embedding_dim, char_lstm_dim, num_layers=1, bidirectional=True)
            init_lstm(self.char_lstm)

        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)
        if pre_word_embeds is not None:
            self.pre_word_embeds = True
            self.word_embeds.weight = nn.Parameter(torch.FloatTensor(pre_word_embeds))
        else:
            self.pre_word_embeds = False

        self.dropout = nn.Dropout(0.5)
        if self.n_cap and self.cap_embedding_dim:
            self.lstm = nn.LSTM(embedding_dim+char_lstm_dim*2+cap_embedding_dim, hidden_dim, bidirectional=True)
        else:
            self.lstm = nn.LSTM(embedding_dim+char_lstm_dim*2, hidden_dim, bidirectional=True)

        init_lstm(self.lstm)
        self.hw_trans = nn.Linear(self.out_channels, self.out_channels)
        self.hw_gate = nn.Linear(self.out_channels, self.out_channels)
        self.h2_h1 = nn.Linear(hidden_dim*2, hidden_dim)
        self.tanh = nn.Tanh()
        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)
        init_linear(self.h2_h1)
        init_linear(self.hidden2tag)
        init_linear(self.hw_gate)
        init_linear(self.hw_trans)

        if self.use_crf:
            self.transitions = nn.Parameter(
                torch.zeros(self.tagset_size, self.tagset_size))
            self.transitions.data[tag_to_ix[START_TAG], :] = -10000
            self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000

    def _score_sentence(self, feats, tags):
        r = torch.LongTensor(range(feats.size()[0]))
        if self.use_gpu:
            r = r.cuda()
            pad_start_tags = torch.cat([torch.cuda.LongTensor([self.tag_to_ix[START_TAG]]), tags])
            pad_stop_tags = torch.cat([tags, torch.cuda.LongTensor([self.tag_to_ix[STOP_TAG]])])
        else:
            pad_start_tags = torch.cat([torch.LongTensor([self.tag_to_ix[START_TAG]]), tags])
            pad_stop_tags = torch.cat([tags, torch.LongTensor([self.tag_to_ix[STOP_TAG]])])

        score = torch.sum(self.transitions[pad_stop_tags, pad_start_tags]) + torch.sum(feats[r, tags])

        return score

    def _get_lstm_features(self, sentence, chars2, caps, chars2_length, d):
        chars_embeds = self.char_embeds(chars2).transpose(0, 1)
        packed = torch.nn.utils.rnn.pack_padded_sequence(chars_embeds, chars2_length)
        lstm_out, _ = self.char_lstm(packed)
        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out)
        outputs = outputs.transpose(0, 1)
        chars_embeds_temp = torch.autograd.Variable(torch.FloatTensor(torch.zeros((outputs.size(0), outputs.size(2)))))
        if self.use_gpu:
            chars_embeds_temp = chars_embeds_temp.cuda()
        for i, index in enumerate(output_lengths):
            chars_embeds_temp[i] = torch.cat((outputs[i, index-1, :self.char_lstm_dim], outputs[i, 0, self.char_lstm_dim:]))
        chars_embeds = chars_embeds_temp.clone()
        for i in range(chars_embeds.size(0)):
            chars_embeds[d[i]] = chars_embeds_temp[i]

        embeds = self.word_embeds(sentence)
        if self.n_cap and self.cap_embedding_dim:
            cap_embedding = self.cap_embeds(caps)

        if self.n_cap and self.cap_embedding_dim:
            embeds = torch.cat((embeds, chars_embeds, cap_embedding), 1)
        else:
            embeds = torch.cat((embeds, chars_embeds), 1)

        embeds = embeds.unsqueeze(1)
        embeds = self.dropout(embeds)
        lstm_out, _ = self.lstm(embeds)
        lstm_out = lstm_out.view(len(sentence), self.hidden_dim*2)
        lstm_out = self.dropout(lstm_out)
        lstm_feats = self.hidden2tag(lstm_out)
        return lstm_feats

    def _forward_alg(self, feats):
        init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.)
        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.
        forward_var = torch.autograd.Variable(init_alphas)
        if self.use_gpu:
            forward_var = forward_var.cuda()
        for feat in feats:
            emit_score = feat.view(-1, 1)
            tag_var = forward_var + self.transitions + emit_score
            max_tag_var, _ = torch.max(tag_var, dim=1)
            tag_var = tag_var - max_tag_var.view(-1, 1)
            forward_var = max_tag_var + torch.log(torch.sum(torch.exp(tag_var), dim=1)).view(1, -1) # ).view(1, -1)
        terminal_var = (forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]).view(1, -1)
        alpha = log_sum_exp(terminal_var)
        return alpha

    def viterbi_decode(self, feats):
        backpointers = []
        init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)
        init_vvars[0][self.tag_to_ix[START_TAG]] = 0
        forward_var = torch.autograd.Variable(init_vvars)
        if self.use_gpu:
            forward_var = forward_var.cuda()
        for feat in feats:
            next_tag_var = forward_var.view(1, -1).expand(self.tagset_size, self.tagset_size) + self.transitions
            _, bptrs_t = torch.max(next_tag_var, dim=1)
            bptrs_t = bptrs_t.squeeze().data.cpu().numpy()
            next_tag_var = next_tag_var.data.cpu().numpy()
            viterbivars_t = next_tag_var[range(len(bptrs_t)), bptrs_t]
            viterbivars_t = torch.autograd.Variable(torch.FloatTensor(viterbivars_t))
            if self.use_gpu:
                viterbivars_t = viterbivars_t.cuda()
            forward_var = viterbivars_t + feat
            backpointers.append(bptrs_t)

        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]
        terminal_var.data[self.tag_to_ix[STOP_TAG]] = -10000.
        terminal_var.data[self.tag_to_ix[START_TAG]] = -10000.
        best_tag_id = argmax(terminal_var.unsqueeze(0))
        path_score = terminal_var[best_tag_id]
        best_path = [best_tag_id]
        for bptrs_t in reversed(backpointers):
            best_tag_id = bptrs_t[best_tag_id]
            best_path.append(best_tag_id)
        start = best_path.pop()
        assert start == self.tag_to_ix[START_TAG]
        best_path.reverse()
        return path_score, best_path

    def neg_log_likelihood(self, sentence, tags, chars2, caps, chars2_length, d):
        feats = self._get_lstm_features(sentence, chars2, caps, chars2_length, d)

        if self.use_crf:
            forward_score = self._forward_alg(feats)
            gold_score = self._score_sentence(feats, tags)
            return forward_score - gold_score
        else:
            tags = torch.autograd.Variable(tags)
            scores = nn.functional.cross_entropy(feats, tags)
            return scores

    def forward(self, sentence, chars, caps, chars2_length, d):
        feats = self._get_lstm_features(sentence, chars, caps, chars2_length, d)
        if self.use_crf:
            score, tag_seq = self.viterbi_decode(feats)
        else:
            score, tag_seq = torch.max(feats, 1)
            tag_seq = list(tag_seq.cpu().data)

        return score, tag_seq

train_sentences, train_labels, train_tags, dev_sentences, dev_labels, dev_tags, test_sentences, test_labels, test_tags = build_sentences()
build_vocab(train_sentences, train_tags, train_labels)

train_data = prepare_dataset(train_sentences, train_labels, word_to_id, char_to_id, label_to_id)
dev_data = prepare_dataset(dev_sentences, dev_labels, word_to_id, char_to_id, label_to_id)
test_data = prepare_dataset(test_sentences, test_labels, word_to_id, char_to_id, label_to_id)

parameters = OrderedDict()
parameters['lower'] = 1
parameters['zeros'] = 1
parameters['char_mode'] = 'LSTM'

pretrained_word_embeddings = {}
#file = open('/content/drive/My Drive/NLP Assignments/Final/data/glove.twitter.27B/glove.twitter.27B.100d.txt', encoding = 'utf-8')
#file = open('/content/drive/My Drive/NLP Assignments/Final/data/glove.6B/glove.6B.200d.txt', encoding = 'utf-8')
file = open('/content/drive/My Drive/NLP Assignments/Final/data/glove.6B.100d.txt', encoding = 'utf-8')
#file = open('/content/drive/My Drive/NLP Assignments/Final/data/glove.6B.50d.txt', encoding = 'utf-8')

i=0
for line in file:
    s = line.strip().split()
    if len(s) == 100 + 1:
    #if len(s) == 50 + 1:
    #if len(s) == 200 + 1:
        pretrained_word_embeddings[s[0]] = np.array([float(i) for i in s[1:]])
    i+=1
file.close()
word_embeddings_init = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_to_id), 100))
#word_embeddings_init = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_to_id), 50))
#word_embeddings_init = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_to_id), 200))

for w in word_to_id:
    if w in pretrained_word_embeddings:
        word_embeddings_init[word_to_id[w]] = pretrained_word_embeddings[w]
    elif w.lower() in pretrained_word_embeddings:
        word_embeddings_init[word_to_id[w]] = pretrained_word_embeddings[w.lower()]

model = BiLSTM_CRF(vocab_size = len(word_to_id),
                   tag_to_ix = label_to_id,
                   embedding_dim = 100, #50, 100
                   hidden_dim = 300, #200
                   char_to_ix = char_to_id,
                   pre_word_embeds = word_embeddings_init,
                   use_crf = 1,
                   char_mode = 'LSTM',
                   n_cap = 4,
                   cap_embedding_dim = 10)
model.train(True)

LEARNING_RATE = 0.05
MOMENTUM = 0.9
EPOCHS=10
optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)

for _ in range(EPOCHS):
    for i, index in tqdm.tqdm(enumerate(np.random.permutation(len(train_data)))):
        data = train_data[index]
        model.zero_grad()

        sentence_in, tags, chars = data['words'], data['tags'], data['chars']
        sentence_in = torch.autograd.Variable(torch.LongTensor(sentence_in))

        chars_sorted = sorted(chars, key=lambda ch: len(ch), reverse=True)
        char_dict = {}
        for i, ci in enumerate(chars):
            for j, cj in enumerate(chars_sorted):
                if ci == cj and not j in char_dict and not i in char_dict.values():
                    char_dict[j] = i
                    continue
        chars_sorted_length = [len(c) for c in chars_sorted]
        chars_upper_bound = max(chars_sorted_length)
        chars_to_vec = np.zeros((len(chars_sorted), chars_upper_bound), dtype='int')
        for i, c in enumerate(chars_sorted):
            chars_to_vec[i, :chars_sorted_length[i]] = c
        chars_to_vec = torch.autograd.Variable(torch.LongTensor(chars_to_vec))
        
        targets, capitals = torch.LongTensor(tags), torch.autograd.Variable(torch.LongTensor(data['caps']))
        
        loss = model.neg_log_likelihood(sentence_in, targets, chars_to_vec, capitals, chars_sorted_length, char_dict)
        loss.backward()
        torch.nn.utils.clip_grad_norm(model.parameters(), 5.0)
        optimizer.step()